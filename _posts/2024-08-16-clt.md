---
layout: archive
title: 'Central Limit Theorem and Normal Approximations'
date: 2024-08-16
permalink: /posts/2024/08/clt/
tags:
  - statistics
  - shiny
classes: wide
---

This post discusses the classical Central Limit Theorem and provides demonstrates its use through Normal approximations of the Binomial and Poisson distributions with a Shiny app.

------------------------------------------------------------------------

# What is the Central Limit Theorem?

The classical Central Limit Theorem (CLT) is a theorem that states that, when certain conditions are met, the sampling distribution of a (normalized) sample mean converges to a (standard) normal distribution, even if the original variables are not normally distributed.

The formal definition of the classical CLT is as follows:

**Central Limit Theorem:** Let $X_1, X_2, \ldots, X_n$ be independent and identically distributed random variables from some distribution with $\text{E}(X_i) = \mu$ and $\text{Var}(X_i) = \sigma^2 < \infty$. Denote $\bar{X}_n$ as the sample mean of the $n$ random variables. Then

$$
\sqrt{n}(\bar{X}_n - \mu) \xrightarrow{d} \text{N}(0, \sigma^2),
$$

which effectively means that as $n$ gets larger, the distribution of $\bar{X}_n$ gets more "normal". This "classical" form of the CLT is actually known as the Lindeberg–Lévy CLT. It is important to note that there are other CLTs with different/relaxed assumptions regarding aspects of independence or identically distributed random variables, but they are beyond the scope of this post.

# Why is the CLT important?

Because the CLT uses known population parameters such as $\mu$ and $\sigma$, which are almost never known in practice when working with real data, the CLT may initially seem like a purely theoretical result without much application. However, when used in conjunction with other important ideas, such as [Slutsky's theorem](https://en.wikipedia.org/wiki/Slutsky's_theorem) and the [Delta method](https://en.wikipedia.org/wiki/Delta_method), the CLT allows us to approximate distributions of many kinds of statistics of interest, including means and variances. 

In short, the CLT is the basis of many statistical procedures and is often used to
* construct confidence intervals
* perform a variety of statistical tests
* understand and approximate the distribution of various statistics


## Example: Binomial approximation

A simple example that demonstrates the CLT is to use the normal distribution to approximate the binomial distribution.

To understand the binomial distribution, let's first define a Bernoulli random variable. A random variable $X$ follows the Bernoulli distribution if:
* $X$ can be either 1 ("success") or 0 ("failure")
* $p$ is the probability of success

When we have a collection of independent and identically distributed Bernoulli random variables $X_1, X_2, \ldots, X_n$, then the sum $Y = \sum_{i=1}^n X_i$ follows a binomial distribution. Specifically, $Y \sim \text{Binomial}(n, p)$.

We can use the CLT to obtain the an approximate distribution for $Y$ as a $\text{N}(np, np(1-p))$ since $\text{E}(Y) = np$ and $\text{Var}(Y) = np(1-p)$.

Here's the shiny app:

<embed src="https://taylor-grimm.shinyapps.io/clt_shiny/" style="width:100%; height: 20vw;">
