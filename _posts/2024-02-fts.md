---
title: 'Functional Time Series'
date: 2024-02-22
permalink: /posts/2024/02/fts/
tags:
  - statistics
  - time series
---

This post gives a brief introduction to functional time series.

- <a href="#notation" id="toc-notation">Notation</a>
- <a href="#review-of-basic-time-series-concepts"
  id="toc-review-of-basic-time-series-concepts">Review of Basic Time
  Series Concepts</a>
- <a href="#functional-autorogressive-process"
  id="toc-functional-autorogressive-process">Functional Autorogressive
  Process</a>
  - <a href="#estimating-phi" id="toc-estimating-phi">Estimating <span
    class="math inline"><em>Φ</em></span></a>
  - <a href="#generating-and-estimating-a-far1-model"
    id="toc-generating-and-estimating-a-far1-model">Generating and
    Estimating a FAR(1) Model</a>
- <a href="#forecasting-with-the-hyndman-ullah-method"
  id="toc-forecasting-with-the-hyndman-ullah-method">Forecasting with the
  Hyndman-Ullah Method</a>
  - <a href="#example-australian-fertility-rates"
    id="toc-example-australian-fertility-rates">Example: Australian
    Fertility Rates</a>
- <a href="#forecasting-with-multivariate-predictors"
  id="toc-forecasting-with-multivariate-predictors">Forecasting with
  Multivariate Predictors</a>
- <a href="#additional-topics" id="toc-additional-topics">Additional
  Topics</a>
  - <a href="#long-run-covariance-function"
    id="toc-long-run-covariance-function">Long-run Covariance Function</a>
  - <a href="#testing-stationarity-of-functional-time-series"
    id="toc-testing-stationarity-of-functional-time-series">Testing
    Stationarity of Functional Time Series</a>
  - <a href="#conditions-for-the-existence-of-the-far1-process"
    id="toc-conditions-for-the-existence-of-the-far1-process">Conditions for
    the Existence of the FAR(1) Process</a>

#### Notation

For this section, the following notation is used:

Let the curves $X_1, X_2, \ldots,$ represent some variable at a given
location, such as pollution levels. Then, we denote $X_n(t)$ as the
level of pollution at time $t$ of day $n$.

Note that $t$ denotes the argument of the function, and $n$ denotes the
time index. So, we denote a time series as $x_n,$ where
$n = 1, 2, \ldots, N$.

## Review of Basic Time Series Concepts

In time series analysis, we seek to quantitatively describe the random
mechanism which generates the data of interest.

A time series model specifies the distribution of random variables
$X_n, n\in\mathbb{Z}$.

AR(1) Model  
The most common basic time series model is the autoregressive process of
order 1, denoted AR(1), which is defined by

$$
X_n - \mu = \varphi(X_{n-1} - \mu) + \varepsilon_n,
$$

where $\{\varepsilon_n\}$ is a sequence of white noise, meaning

$$
	ext{E}[\varepsilon_n] = 0, \quad 	ext{Var}[\varepsilon_n] = \sigma^2, \quad 	ext{Cov}(\varepsilon_n, \varepsilon_{n+h}) = 0 \text{ if } h \ne 0.
$$

Stationarity  
We say that a time series $\{X_n, n \in \mathbb{Z} \}$ is *stationary*
if $	ext{E}(X_n)$ and $	ext{Cov}(X_n, X_{n+h})$ do not
depend on the value of $n$.

An AR(1) model is stationary if and only if $|\varphi| < 1$.

Autocovariance and Autocorrelation  
We define the autocovariance function as

$$
\gamma_h = 	ext{Cov}(X_n, X_{n+h}), \quad h = 0, 1, 2,\ldots.
$$

The autocovariance function is only defined for a stationary model, and
we only use nonnegative lags $h$ since

$$
\gamma_{-h} = 	ext{Cov}(X_n, X_{n-h}) = 	ext{Cov}(X_{n-h}, X_n) = 	ext{Cov}(X_n, X_{n+h}) = \gamma_h.
$$

The autocorrelation function is defined by

$$
\rho_h = 	ext{Corr}(X_n, X_{n+h}) = \frac{\gamma_h}{\gamma_0}, \quad h = 0,1,2,\ldots.
$$

Given observations $x_1, x_2, \ldots, x_N$ we estimate the
autocovariance function by using the *sample* autocovariance functions

$$
\hat{\gamma}_h = \frac{1}{N}\sum_{n=1}^{N-h}(x_n - \bar{x}))(x_{n+h}-\bar{x}), \quad \bar{x} = \frac{1}{N}\sum_{i=1}^N x_n.
$$

The denominator $N$ in the formula for $\hat{\gamma}_h$ is often
replaced by $N-h$.

The autocorrelation function is estimated by using the sample
autocovariance function to produce

$$
\hat{\rho}_h = \frac{\hat{\gamma}_h}{\hat{\gamma}_0}.
$$

Prediction  
For an AR(1) model with $|\varphi| < 1$, the *one-step-ahead* prediction
(or forecast) is

$$
\hat{X}_{n+1} = \hat{\mu} + \hat{\varphi}(X_n - \hat{\mu}).
$$

## Functional Autorogressive Process

Recall that $L^2$ denotes the set of all square integrable functions.

A sequence $\{X_n, -\infty < n < \infty \}$ of mean zero elements of
$L^2$ follows a *functional* AR(1), denoted a FAR(1), model if

$$
X_n = \Phi(X_{n-1}) + \varepsilon_n,
$$

where $\Phi: L^2 \to L^2$ is an operator that transforms a function into
another function, and $\{\varepsilon_n, -\infty < n < \infty \}$ is a
sequence of iid mean zero errors.

We define the operator $\Phi$ as the following integral operator:

$$
\Phi(x)(t) = \int \varphi(t,s)x(s)ds, \quad x \in L^2,
$$

which allows us to rewrite the FAR(1) model as

$$
X_n(t) = \int \varphi(t,s) X_{n-1}(s)ds + \varepsilon_n(t).
$$

### Estimating $\Phi$

Scalar Case  
In the univariate scalar case, we assume that $|\varphi|<1$, and we can
multiply the AR(1) equation $X_n = \varphi X_{n-1} + \varepsilon_n$ by
$X_{n-1}$ and then take the expected value, which yields

$$
	ext{E}[X_{n-1}X_n] = \gamma_1 = 	ext{E}[\varphi X_{n-1}X_{n-1}] = \varphi \gamma_0.
$$

Then, we can solve for $\varphi$ to obtain the usual estimator of
$\varphi$ as $\hat{\varphi} = \hat{\gamma}_1 / \hat{\gamma}_0$, since we
replace the autocovariances with the sample autocovariances.

Functional Case  
In the functional model, we apply a similar approach. We have

$$
	ext{E}[\langle X_n, x \rangle X_{n-1}] = 	ext{E}[\langle \Phi(X_{n-1}), x\rangle X_{n-1}], \quad x \in L^2.
$$

Let the lag-1 autocovariance operator be defined as

$$
C_1(x) = 	ext{E}[\langle X_n, x \rangle X_{n+1}]
$$

and denote with the superscript $\cdot^*$ the adjoint operator. Then,
$C_1^* = C\Phi^*$ because
$C_1^* = 	ext{E}[\langle X_n,x \rangle X_{n-1}]$. Therefore, we
have

$$
C_1 = \Phi C,
$$

which is similar to what we saw in the univariate scalar case.

Note  
We define the adjoint operator $A^*$ as satisfying
${\langle Ax,y\rangle =\langle x,A^{*}y\rangle}$

We can now solve for $\Phi$ as $\Phi = C_1C^{-1}$. However, $C^{-1}$ is
not defined on the whole space $L^2$ (see pg. 146-147 of the textbook).

Let $v_j$ be the eigenfunctions of $C$, and $\lambda_j$ be the
corresponding eigenvalues. We obtain the following estimator for $\Phi$:

$$
\hat{\Phi}_p(x) = \frac{1}{N-1}\sum_{k=1}^{N-1}\sum_{j=1}^p\sum_{i=1}^p \hat{\lambda}_j^{-1} \langle x, \hat{v}_j\rangle \langle X_k, \hat{v}_j \rangle \langle X_{k+1}, \hat{v}_i \rangle \hat{v}_i.
$$

The expression above results in

$$
\hat{\varphi}_p(t,s) = \frac{1}{N-1}\sum_{k=1}^{N-1}\sum_{j=1}^p\sum_{i=1}^p \hat{\lambda}_j^{-1} \langle X_k, \hat{v}_j\rangle \langle X_{k+1}, \hat{v}_i \rangle \hat{v}_j(s) \hat{v}_i(t).
$$

### Generating and Estimating a FAR(1) Model

Generation  
We can use the `fda` package to generate a realization of a FAR(1)
process and to estimate the model.

Let $N=200$, $\varphi(s,t) = \alpha s t$, $\alpha = 9/4$, and use

$$
\varepsilon_n(t) = Z_{n1}\sin(\pi t) + \frac{1}{2}Z_{n2}\cos(2\pi t), \quad t \in [0, 1].
$$

where $Z_{n1}$ and $Z_{n2}$ are independent standard normal. For
burn-in, we generate 250 functions and discard the first 50.

``` r
library(fda)

m <- 100 # each function is observed at m+1 points, including 0 and 1
burnin <- 50 # the first 50 functions are a burn in period
N <- 200 # number of functions to simulate
N1 <- N + burnin
alpha <- 9/4

# Create 2 matrices, whose entries are 0s.
# Each column represents one function.
X <- matrix(rep(0, (m+1) * N1), m+1, N1)
epsilon <- matrix(rep(0, (m + 1) * N1), m+1, N1)

set.seed(24)
epsilon[,1] <- rnorm(1) * sin(pi * (0:m/m)) + 0.5 * rnorm(1) *
  cos(2 * pi * (0:m/m))
# the following loop simulates FAR(1).
for(i in 2:N1){
epsilon[,i] <- rnorm(1) * sin(pi * (0:m/m)) + 0.5 * rnorm(1) *
  cos(2 * pi * (0:m/m))
X[,i] <- alpha * (1/m)^2 * sum((1:m) * X[2:(m+1), i-1])*(0:m/m) +
  epsilon[,i]
}

X <- X[, -(1:burnin)] # Remove the burn in period functions

last <- 10
plot.ts(c(X[, (N-last+1):N]), ylim = c(min(X[, (N-last):N]) - 0.5,
                                       0.5 + max(X[, (N-last):N])),
        axes = F, xlab = "", ylab = "", lwd = 2,
        main = 'Last 10 Observations from Simulated FAR(1) Process')
axis(2); axis(1, tick = F, labels = F); abline(h = 0)
abline(v = seq(0, last*(m+1), by = m+1), lty=2); box()
```

![](functional_time_series_notes_files/figure-gfm/unnamed-chunk-1-1.png)

*Note: We can also use the `far` package to easily generate and estimate
FAR processes. (See problem 8.6 in the book)*

Now that we’ve generated a FAR(1) process, we can create functional
objects for each functional observation.

``` r
basisfd <- 10 # number of basis functions to represent each functional observation
basis <- create.bspline.basis(c(0, 1), nbasis = basisfd, norder = 4)
fdX = Data2fd(argvals = 0:m/m, X, basis)
```

The functional objects can now be plotted as usual.

``` r
plot(fdX, col = viridis::viridis(200), lty = 1)
```

![](functional_time_series_notes_files/figure-gfm/unnamed-chunk-3-1.png)

    [1] "done"

Estimation  
Recall that our estimator is

$$
\hat{\varphi}_p(t,s) = \frac{1}{N-1}\sum_{k=1}^{N-1}\sum_{j=1}^p\sum_{i=1}^p \hat{\lambda}_j^{-1} \langle X_k, \hat{v}_j\rangle \langle X_{k+1}, \hat{v}_i \rangle \hat{v}_j(s) \hat{v}_i(t).
$$

So, we need to obtain the EFPC’s. We see that the first component
explains 80% of variance, and that the first 2 explain 96% of variance.

``` r
p <- 4 # number of EFPC's
fdXpca <- pca.fd(fdX, nharm = p)
eigenvalues <- fdXpca$values; scoresX <- fdXpca$scores
# jth column of scoresX contains scores of the jth EFPC
harmonicsX <- fdXpca$harmonics # extract the EFPC's

varianceprop <- fdXpca$varprop #proportion of variance explained by the EFP's
round(varianceprop*100,0)
```

    [1] 80 16  4  0

Then, we can plot the true surface $\varphi(s,t)$ and the estimates when
using $p = 1, 2,$ or 3 PC’s (see pg. 168-169 for code).

![](functional_time_series_notes_files/figure-gfm/unnamed-chunk-5-1.png)

## Forecasting with the Hyndman-Ullah Method

This method was developed specifically for forecasting of mortality
rates, but it is apparently popular for general functional time series
forecasting.

Unlike the FAR(1) model, this method has

- no stationarity assumption
- no assumed specific autocorrelation structure.

We begin with $N$ observations of a “sufficiently smooth” functional
time series. Consider the (truncated) approximation

$$
X_n^{(J)}(t) = \hat{\mu}(t) + \sum_{j=1}^J \hat{\xi}_{n,j} \hat{v}_j(t),
$$

where the $\hat{\xi}_{n,j}$’s are the estimated PC scores.

Once we’ve selected a truncation level $J$ (see pg. 148), we can then
forecast the curve $X_{N+h}$ by

$$
X_{N+h}^{(J)}(t) = \hat{\mu}(t) + \sum_{j=1}^J \hat{\xi}_{N+h|N,j}\hat{v}_j(t).
$$

Since we have $\hat{\mu}(t) = N^{-1}\sum_{n=1}^N X_n(t)$, all we have to
do is predict the $\hat{\xi}_{N+h,j}$’s.

To estimate the $\hat{\xi}_{N+h,j}$’s, we decompose each smooth curve
via functional PCA.

- Forecast each score $h$ steps ahead (individually) using univariate
  time series models (e.g. ARIMA).

### Example: Australian Fertility Rates

We have 95 observations corresponding to fertility rates between ages 15
and 49($t \in [15,49]$) from 1921 to 2006.

- number of live births during the year, according to the age of the
  mother, per 1000 of the female resident population of the same age at
  June 30th.

Dimensions of the data are below.

- 35 rows (each age) and 95 columns (each year)

Each column represents a function with the associated 35 fertility
values.

``` r
Australiafertility$y |> str()
```

     num [1:35, 1:95] 1.75 6.85 18.29 39.93 67.58 ...
     - attr(*, "dimnames")=List of 2
      ..$ : chr [1:35] "15" "16" "17" "18" ...
      ..$ : chr [1:95] "1921" "1922" "1923" "1924" ...

The raw and pre-smoothed functions are shown below.

- highest fertility rates between around 1950-1970 (baby boom)

``` r
par(mfrow=c(2,1), mar = c(4,4,2,1))
plot(Australiafertility, main = 'Raw Age-specific Fertility Rates')
plot(Australiasmoothfertility, main = 'Smoothed Age-specific Fertility Rates')
```

![](functional_time_series_notes_files/figure-gfm/unnamed-chunk-7-1.png)

There are a few ways to implement the HU forecasting method. One way is
to use the `ftsa` (“Functional Time Series Analysis”) package, written
by Hyndman.

The plots below compares the HU method described above with a recursive
method.

HU method

- forecast the $X_{N+1}, \ldots, X_{N+h}$ values all at once using the
  true data $X_1, \ldots, X_N$.

Recursive method

1.  Produce one-step-ahead predictions $\hat{X}_{N+1|N}$, treat them as
    real observations.

2.  Predict $X_{N+2}$, and so on until we obtain an $h$ step ahead
    forecast.

``` r
library("ftsa"); par(mfrow=c(2,1), mar = c(4,4,2,1))
# Plot the historical data in gray
plot(Australiasmoothfertility, col = gray(0.8), xlab = "Age",
     ylab = "Births per 1,000 females",
     main = "Forecasted fertility rates (2007-2026)")

# Plot the forecasts in rainbow color
# We must first create a functional time series (ftsm) model
# and determine how many PC's to fit
aus_ftsm <- ftsm(Australiasmoothfertility, order = 2)
# Default forecasting uses an exponential smoothing method ("ets")
# Here, I use an ARIMA model, forecasting 20 steps (years) ahead
plot(forecast(aus_ftsm, h = 20, method = 'arima'), add = TRUE)
legend("topright", c("2007", "2026"), col = c("red", "blue"), lty = 1)

# Repeat the above steps but use recursive forecasts
plot(Australiasmoothfertility,col=gray(0.8),xlab="Age",ylab =
       "Births per 1,000 females",main = "Recursive Forecasts")
plot(ftsmiterativeforecasts(Australiasmoothfertility,
                            components = 2, iteration = 20), add = TRUE)
legend("topright", c("2007", "2026"), col = c("red", "blue"),
       lty = 1)
```

![](functional_time_series_notes_files/figure-gfm/unnamed-chunk-8-1.png)

## Forecasting with Multivariate Predictors

Above, we used the HU method to forecast the PC scores $\hat{\xi}_{n,j}$
individually in order to obtain the $h$ step ahead forecast $X_{N+h}$.
Now, we consider forecasting the vector of scores

$$
\Xi_n^{(J)} = [\hat{\xi}_{n,1}, \hat{\xi}_{n,2}, \ldots, \hat{\xi}_{n,J}]^T
$$

simultaneously to obtain the forecasted vector $\Xi_{N+h}^{(J)}$.

This can be done by simply applying a multivariate time series
prediction technique (such as a vector autoregressive model, or VAR) to
the vector of scores. This can be done using the `farforecast` function
in the `fsta` package in R.

The multivariate forecast is seen below, with the one-step-ahead
forecast shown with black dots, and the last (30-steps-ahead) forecast
shown in green. As the number of steps increases, the forecasts converge
to the overall mean function of the functional time series, which we
expect to happen with predictions of stationary series.

``` r
# 30 step ahead forecast
multi_forecast_sqrt_pm10 = farforecast(object = fts(pm_10_GR$x, pm_10_GR_sqrt$y), 
    h = 30, # how many steps ahead to forecast
    Dmax_value = 5, # maximum number of principal components considered
    Pmax_value = 3) # maximum order of VAR model considered

# one step ahead
oneStep_forecast_sqrt_pm10 = farforecast(object = fts(pm_10_GR$x, pm_10_GR_sqrt$y), 
    h = 1, # how many steps ahead to forecast
    Dmax_value = 5,
    Pmax_value = 3)

# Plot the forecasts
plot(multi_forecast_sqrt_pm10$point_fore, xlab = 'Hour',
     ylab = 'Square root of pm10')
lines(oneStep_forecast_sqrt_pm10$point_fore, lwd=3, lty=3)
```

![](functional_time_series_notes_files/figure-gfm/unnamed-chunk-9-1.png)

``` r
# The code below is the code from the textbook, but it does not run anymore
# (probably due to changes in the ftsa package)
#
# library(vars)
# 
# x = seq(0, 23.5, by = 0.5)
# multi_forecast_sqrt_pm10 = farforecast(ftsm(pm_10_GR_sqrt), h = 30, PI=F)
# plot(multi_forecast_sqrt_pm10, ylim=c(5.2,7.5), xlab="Hour", ylab="Square root of pm10", lw=2)
# oneStep_forecast_sqrt_pm10 = farforecast(ftsm(pm_10_GR_sqrt), h=1)
# lines(oneStep_forecast_sqrt_pm10, lwd=3, lty=3)
```

## Additional Topics

### Long-run Covariance Function

In time series, we sometimes use the *long-run variance* (LRV) instead
of the usual sample variance.

Scalar Case :For a scalar stationary process $\{X_n\}$ with
autocovariance $\gamma_h$, the LRV is defined as

$$
\sigma^2 = \sum_{h=-\infty}^\infty \gamma_h = \gamma_0 + 2 \sum_{h=1}^\infty \gamma_h.
$$

Details for this definition can be found on page 156 of the textbook.

The LRV is commonly estimated by

$$
\hat{\sigma}^2 = \hat{\sigma}^2(K,q) = \sum_{h=-N+1}^{N+1} K\left(\frac{h}{q}\right)\hat{\gamma}_h,
$$

where $K$ is called the weight function (or lag window), $q$ is the
bandwidth, and $\hat{\gamma}_h$ is the sample autocovariance. Additional
details about $K$ and $q$ can be found on page 157 of the textbook.

Functional Case  
For a stationary functional sequence, the *long-run covariance function*
(LRCF) is defined by

$$
\sigma(t,s) = \sum_{h=-\infty}^\infty \gamma_h(t,s) = \gamma_0(t,s) + 2\sum_{h=1}^\infty (\gamma_h(t,s) + \gamma_h(s,t)).
$$

Similar to the scalar case, additional theoretical details regarding the
derivation above can be found on page 158 of the textbook.

The estimator of the LRCF is

$$
\hat{\sigma}(t,s) =  \sum_{h=-N+1}^{N-1} K \left(\frac{h}{q}\right)\hat{\gamma}_h(t,s).
$$

Further expansion of this estimator can be found on pages 158 and 159 of
the textbook.

### Testing Stationarity of Functional Time Series

As we have seen, many procedures assume stationarity of the functional
time series. In order to verify the assumption, we can conduct a test of
the stationarity.

Our null hypothesis is

$$
H_0: X_i(t) = \mu(t) + \eta_i(t),
$$

where $\{\eta_i(t)\}$ is a mean zero strictly stationary sequence, and
$\mu(t)$ is the (unknown) mean function.

To test this hypothesis, we need to define an alternative hypothesis and
then compute a test statistic.

One alternative hypothesis is the change point alternative

$$
H_{A,1}: X_i(t) = \mu(t) + \delta(t)I \{i > k^*\} + \eta_i(t),
$$

where the *change point* $1\le k^* \le N$ is an unknown integer.
Specifically, we are looking to see if the mean function of the curves
changes at time $k^*$ from $\mu(t)$ to $\mu(t) + \delta(t)$.

Another alternative hypothesis is the random walk

$$
H_{A,2}: X_i(t) = \mu(t) + \sum_{\ell=1}^i u_\ell(t),
$$

where $\{u_\ell\}$ is stationary with mean zero (and often assumed to be
iid).

Theoretical details about deriving the test statistics $\widehat{T}_N$
and $\widehat{T}_N^0(d)$ involve the LRCF, Brownian bridges, and
convergence, and can be found on pages 160-163 of the textbook.

Implementing the monte carlo tests for stationarity using each of these
test statistics can be done using the `T_stationary` function in the
`ftsa` package.

``` r
T_stationary(pm_10_GR_sqrt$y)
```


    Monte Carlo test of stationarity of a functional time series

    null hypothesis: the series is stationary

    p-value = 0.07 
    N (number of functions) = 182 
    number of MC replications = 1000

``` r
T_stationary(pm_10_GR_sqrt$y, J=100, MC_rep = 5000, h = 20, pivotal = T)
```


    Pivotal test of stationarity for a functional time series

    null hypothesis: the series is stationary

    p-value = 0.1234 
    N (number of functions) = 182 
    number of MC replications = 5000

### Conditions for the Existence of the FAR(1) Process

Theoretical details regarding conditions for the existence of the FAR(1)
process can be found on pages 171-173 of the textbook.
